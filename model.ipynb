{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from noisereduce.torchgate import TorchGate as TG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Background Noise (Torchgate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# # Create TorchGating instance\n",
    "# tg = TG(sr=8000, nonstationary=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "csv_file_path = 'data/ground_truth_clip_labels.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (DataFrame): DataFrame containing 'Show', 'EpId', 'ClipId', and 'y'.\n",
    "            root_dir (str): Directory with all the audio files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Construct file path\n",
    "        audio_name = os.path.join(self.root_dir, f\"{self.dataframe.iloc[idx]['Show']}_{self.dataframe.iloc[idx]['EpId']}_{self.dataframe.iloc[idx]['ClipId']}.wav\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "\n",
    "        # Get label\n",
    "        label = self.dataframe.iloc[idx]['y']\n",
    "\n",
    "        sample = {'audio': waveform, 'sample_rate': sample_rate, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Specify the directory containing the audio files\n",
    "audio_dir = 'data/clips/'\n",
    "\n",
    "# Initialize dataset\n",
    "audio_dataset = AudioDataset(dataframe=df, root_dir=audio_dir)\n",
    "\n",
    "# Example of using DataLoader to create iterable data loader\n",
    "# data_loader = DataLoader(audio_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f31b2916c746c6aef3d2f40295aec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  49%|####8     | 73.4M/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n",
    "\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "token = \"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\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, token=token)\n",
    "model = WhisperForAudioClassification.from_pretrained(model_id, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have already initialized 'audio_dataset' and 'feature_extractor' as shown in previous code\n",
    "\n",
    "# Define the preprocessing transform as a PyTorch nn.Module\n",
    "class PreprocessAudio(torch.nn.Module):\n",
    "    def __init__(self, required_rate):\n",
    "        super(PreprocessAudio, self).__init__()\n",
    "        self.required_rate = required_rate\n",
    "\n",
    "    def forward(self, sample):\n",
    "        waveform, sample_rate = sample['audio'], sample['sample_rate']\n",
    "        \n",
    "        # Use the feature extractor\n",
    "        return feature_extractor(waveform.squeeze(0), sampling_rate=self.required_rate, return_tensors=\"pt\", padding=True, max_length=16000, truncation=True)\n",
    "\n",
    "# Add the preprocessing step to the dataset class\n",
    "audio_dataset.transform = PreprocessAudio(required_rate=feature_extractor.sampling_rate)\n",
    "\n",
    "# Define a DataLoader\n",
    "data_loader = DataLoader(audio_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "# Example loop to process batches\n",
    "for batch in data_loader:\n",
    "    inputs = [sample['input_values'] for sample in batch]  # Extract processed features\n",
    "    # You can continue here to feed 'inputs' to your model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
